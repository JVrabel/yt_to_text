{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip install --upgrade pytube\n",
    "# import pytube\n",
    "# import re\n",
    "\n",
    "# with open('/Users/kubov/.conda/envs/myenv/lib/site-packages/pytube/cipher.py', 'r') as file:\n",
    "#     filedata = file.read()\n",
    "#     filedata = re.sub(r'transform_plan_raw = find_object_from_startpoint(raw_code, match.span()[1] - 1)', r'transform_plan_raw = js', filedata)\n",
    "\n",
    "# with open('/Users/kubov/.conda/envs/myenv/lib/site-packages/pytube/cipher.py', 'w') as file:\n",
    "#     file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytube\n",
    "from src.audio_downloader import audio_down\n",
    "\n",
    "audio_down('https://www.youtube.com/watch?v=2zW33LfffPc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/whisper.git -q\n",
    "import whisper\n",
    "import ffmpeg\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "text = model.transcribe(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" GPT4 is coming out this week. This week, not really. And also Samsung isn't trouble because they fake the moon. My name is Yannick and this is ML News. INTRO GPT4 is apparently coming out this week. Hi, I'm Yannick. You may recognize me from the video you're watching. This week was certainly one of the biggest weeks in AI. Google announced an API to their huge palm models and also an integration into their workspace features, which means docs, presentations, spreadsheets, and so on, AI augmented. Microsoft did the same announcing co-pilot for office, which means that soon you'll be able to write a word document or make a PowerPoint presentation and be supported by generative AI. And THROPIC announced their Claude model, which is a chatbot that they have trained and is said to be very good. On the same time, Lama has been made to run on like old smartphones. Maybe someone's toaster. So this is these giant language models. People are taking them and they are doing incredible things with it. And of course, on top of it all, GPT4 was announced. So the new model by OpenAI, it's apparently a lot better than the old GPT3.5 models or chat GPT models. And it's a giant announcement. And this guy right here, he's recording this on Monday morning and he has no clue of that. In fact, he's going to claim that he believes that GPT4 will not be announced this week and he'll be very, very smug about it. So I thought I won't spare you this and I'll let you have this. I cut it together a little bit. But I believe in making falsifiable predictions and I was falsified in this case. We can dive into all the big news next week, as I said. All of this is before that. So my main news here is that Samsung fakes the moon. Not the moon landing, the moon, which is also pretty cool. But enjoy the current buzz of AI. It's a fantastic world. I'm sure it's going to stay exciting, remain exciting and continue even more glorious. That's it for me. Enjoy the video. I'll see you. This article in Heizer Online here, this is the original article I could find. It's in German, but I'll do my best to translate. They say GPT4 will appear next week and that was last week. So this week, the CTL of Microsoft Germany, so a high ranking Microsoft Germany employees said that GPT4 was immediate before release, saying we will next week present GPT4. There we have multimodal models. They offer very different possibilities, for example, video. This is a strong statement, obviously, and the whole media landscape is going absolutely crazy here. But if you happen to think, wait a minute, wait a minute, it's kind of weird that like some German employees of Microsoft are making the announcement for GPT4, one of the most highly anticipated releases in the AI worlds for the last two years or so. It's kind of weird. It's kind of weird. It's not open AI. For every other one of their product, they release like a big blog post, with shiny examples and whatnot, and that's their announcement of the thing and they go all out. No, it's like Microsoft officials, not open AI, Microsoft officials. In Germany, not even an English-speaking event. If you're a bit skeptical, so am I. My guess, my guess is that it's very probable this person misspoke. This person meant something else, not GPT4. In fact, we're going to see a later in this episode, visual chat GPT, which can interact with text and with images and so on, and maybe video is going to be added to that a little bit. Two, no offense to like this person, I'm sure they're doing great job, but it'd be super weird if they were the one to announce this. Now, there is a bit of a smaller chance. Highest chance, misspoke. Smaller chance that this person kind of blabbed something out, they shouldn't, and there is a teeny tiny chance that this was the actual announcement. So my prediction is, there will be no GPT4 this week. [♪ INTRO MUSIC PLAYING [♪ Gans are making a return. Generative adversarial networks were the absolute hype when I started my PhD around 2015-2016, and they're making a comeback. So Gans for a long time have been the sort of state of the art in image generation. They were fast, they were super crisp, paired to variation law to encoders, which were the alternative back then. Gans were the thing, and now recently they've been replaced by diffusion models, which tended to have better quality images and also be steerable via something like text. Now Gans are making a comeback. So this here is Giga GAN, the paper is called scaling up Gans for text to image synthesis. And the pictures here just look beautiful, and you can see they're all created from images. So this augments Gans in a way, so you can also input a piece of text and then have something be produced. And the cool thing is, given that they're Gans, you retain all these abilities like latent space interpolations. Also, what this paper does is they do a style GAN approach, which means that at different resolutions of the image, so they have coarse grain generation on top of that, they have more finer grain generation and so on. If you know, for example, a Laplace pyramid, it's very similar concept, they can apply different conditioning information on the different levels, as I said, like style GAN. Oh yeah, they also paired this with an up sampler. So this is what the up sampler does. This is what the GAN would produce. And then after the up sampler, it looks absolutely beautiful. As I said, the architecture is right here. Generator architecture, you can see there is a lot of tricks in here. So it starts with a free train text encoder. They take that from clip because clip is already trained to pair text and images on top of that, they learn a small encoder and then they use that both as conditioning information, but also as kind of input. It gets very complicated in the exact details. I don't want to go in here, but as I said, they can do at different scales of resolution and they have this interpolation. So for example, they can say we generate a teddy bear on a tabletop and then at the finer grain resolution, they can say something like, ah, we want it to be in crochet, we want it to be made of fur, we want it to be made of denim. And then you can see the teddy bear at the finer grain scale gets that conditioning information. So you'll get a teddy bear made out of fur, for example. Very nice, very controllable and very cool. So there are a lot of possibilities that open up here with models like this. And it's cool to see that GANs are making a comeback because in a diffusion model, I really need to do this step-by-step diffusion. There's some tricks to speed it up, but again, can just shuck a boom, produce that image. The bitter lesson again is that apparently scale is just the thing you need, like you need scale, you need a lot of parameters. And then pretty much any approach can be made to work. But the cool thing is that I like when new paradigms come around, even though GANs have been around since 2014. And some people say since the 90s, I welcome this development and is going to open up to a cool new research area. And I hope with super fast image generation, given by GANs, we have very new possibilities to create experiences, to create applications, to push the state of the art. So very cool. I just thought I throw this in here, chopda.i.com. You can make a recipe. So you say, okay, I have some chicken. It's okay, I have some chicken. And I have some butter. And I have some parsley. I don't know how to spell that. And I have some some rusty nails. I also have some benzo dia zine. Fianzo dia zine. And yeah, and I have nothing, of course. Okay, so I look here and let's see what it gives. So apparently this is supposed to give me a recipe. Let's see whether it works. It's an age old idea. Sorry, I cannot provide the recipe that it includes rusty nails and benzo dia zine. As they're not edible and greedy, that is not true. Okay, I record though. Okay, in any case, it's something, something need to play around. Very cool. Thank you, Helm. Here is a reddit thread. And it carries a pretty serious accusation, I want to say. It's called Samsung Space Zoom. Moonshots are fake. And here is the proof. So this person has a picked up on a debate that has actually been going on for a while. So previously, people have already raised this issue a little bit, but what were dismissed largely? And now this person seems to gather some steam, gather some support. So what is this about? This company called Samsung, they make phones. And specifically, they make phones and they claim the phones have very good cameras. And they also put some AI models into the phones or into the cameras. I'm not sure where the models sit. Probably in the phones. They try to make your pictures that you take with the camera as nice as possible. Now, a lot of companies, I guess, pretty much every single smartphone nowadays does this. But Samsung seems to have a specific affection for sort of pictures of the sky or pictures of the night sky. So what they do is they try to enhance this a lot. What this person now has done is they've taken this particular picture right here. This is a picture of them who not taken with the smartphone. This is, I guess, a NASA picture of the moon. They have blurred it. So they've applied a layer of Gaussian blur to it. So this is now the picture. It's very blurred. You know, this is it. This is the upscaled variant of it. And then they've taken their phone and they have taken a picture of their screen showing the blurred image. Okay. So instead of pointing at the sky, they actually point it at the, they actually point it at the screen as far as I can understand. And this is the picture that comes out of that. Now, as you may see, it's quite different. So in effect, there is information here that is not in the original. So there is no way the camera can actually gather this image information. People previously already said that the moon\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the transcribe\n",
    "text['text'][0:len(text['text'])//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
